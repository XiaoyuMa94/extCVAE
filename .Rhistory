# part2 = log_v$sum()                                                                        # p(v_t|theta_t), p(theta_t)
part4 = Epsilon$pow(2)$sum()/2 + Epsilon_prime$pow(2)$sum()/2 + sigma_sq_vec$log()$sum() + sigma_sq_vec_prime$log()$sum()
res <- part1 + part2 + part4
### -------- compute loss --------
loss <- (res/(n.s*n.t))
if(!is.finite(loss$item())) break
if (t %% 1 == 0)
cat("Epoch: ", t, "   ELBO: ", loss$item(), "\n") # we want to maximize
if (as.numeric(torch_isnan(loss))==1) break
### -------- Backpropagation --------
# compute gradient of loss w.r.t. all tensors with requires_grad = TRUE
loss$backward()
# w_1$grad$argmax(dim=2)
### -------- Update weights --------
# Wrap in with_no_grad() because this is a part we DON'T
# want to record for automatic gradient computation
with_no_grad({
# if(t>190 & learning_rate> -1e-7) {
#   learning_rate <- -1e-2
#   cat('Learing rate is now ', learning_rate, '\n')
# }
# if(t>2 & abs(loss$item()-old_loss)<0.002)  {
#   learning_rate <- learning_rate*alpha_v*0.1;
#   cat('Learing rate is now ', learning_rate, '\n')
# }
old_loss <- loss$item()
w_1_velocity <- alpha_v*w_1_velocity - learning_rate*w_1$grad
w_1$add_(w_1_velocity)
w_2_velocity <- alpha_v*w_2_velocity - learning_rate*w_2$grad
w_2$add_(w_2_velocity)
w_3_velocity <- alpha_v*w_3_velocity - learning_rate*w_3$grad
w_3$add_(w_3_velocity)
w_4_velocity <- alpha_v*w_4_velocity - learning_rate*w_4$grad
w_4$add_(w_4_velocity)
b_1_velocity <- alpha_v*b_1_velocity - learning_rate*b_1$grad
b_1$add_(b_1_velocity)
b_2_velocity <- alpha_v*b_2_velocity - learning_rate*b_2$grad
b_2$add_(b_2_velocity)
b_3_velocity <- alpha_v*b_3_velocity - learning_rate*b_3$grad
b_3$add_(b_3_velocity)
b_4_velocity <- alpha_v*b_4_velocity - learning_rate*b_4$grad
b_4$add_(b_4_velocity)
w_1_prime_velocity <- alpha_v*w_1_prime_velocity - learning_rate*w_1_prime$grad
w_1_prime$add_(w_1_prime_velocity)
w_2_prime_velocity <- alpha_v*w_2_prime_velocity - learning_rate*w_2_prime$grad
w_2_prime$add_(w_2_prime_velocity)
w_3_prime_velocity <- alpha_v*w_3_prime_velocity - learning_rate*w_3_prime$grad
w_3_prime$add_(w_3_prime_velocity)
w_4_prime_velocity <- alpha_v*w_4_prime_velocity - learning_rate*w_4_prime$grad
w_4_prime$add_(w_4_prime_velocity)
b_1_prime_velocity <- alpha_v*b_1_prime_velocity - learning_rate*b_1_prime$grad
b_1_prime$add_(b_1_prime_velocity)
b_2_prime_velocity <- alpha_v*b_2_prime_velocity - learning_rate*b_2_prime$grad
b_2_prime$add_(b_2_prime_velocity)
b_3_prime_velocity <- alpha_v*b_3_prime_velocity - learning_rate*b_3_prime$grad
b_3_prime$add_(b_3_prime_velocity)
b_4_prime_velocity <- alpha_v*b_4_prime_velocity - learning_rate*b_4_prime$grad
b_4_prime$add_(b_4_prime_velocity)
w_5_velocity <- alpha_v*w_5_velocity - learning_rate*w_5$grad
w_5$add_(w_5_velocity)
w_6_velocity <- alpha_v*w_6_velocity - learning_rate*w_6$grad
w_6$add_(w_6_velocity)
w_7_velocity <- alpha_v*w_7_velocity - learning_rate*w_7$grad
w_7$add_(w_7_velocity)
b_5_velocity <- alpha_v*b_5_velocity - learning_rate*b_5$grad
b_5$add_(b_5_velocity)
b_6_velocity <- alpha_v*b_6_velocity - learning_rate*b_6$grad
b_6$add_(b_6_velocity)
b_7_velocity <- alpha_v*b_7_velocity - learning_rate*b_7$grad
b_7$add_(b_7_velocity)
# Zero gradients after every pass, as they'd accumulate otherwise
w_1$grad$zero_()
w_2$grad$zero_()
w_3$grad$zero_()
w_4$grad$zero_()
b_1$grad$zero_()
b_2$grad$zero_()
b_3$grad$zero_()
b_4$grad$zero_()
w_1_prime$grad$zero_()
w_2_prime$grad$zero_()
w_3_prime$grad$zero_()
w_4_prime$grad$zero_()
b_1_prime$grad$zero_()
b_2_prime$grad$zero_()
b_3_prime$grad$zero_()
b_4_prime$grad$zero_()
w_5$grad$zero_()
w_6$grad$zero_()
w_7$grad$zero_()
b_5$grad$zero_()
b_6$grad$zero_()
b_7$grad$zero_()
})
}
learning_rate <- -2e-11
for (t in 1:niter) {
### -------- Forward pass --------
Epsilon <- t(0.1+abs(mvtnorm::rmvnorm(n.t, mean=rep(0, k), sigma = diag(rep(1, k)))))
Epsilon_prime <- t(mvtnorm::rmvnorm(n.t, mean=rep(0, k), sigma = diag(rep(1, k))))
Epsilon <- torch_tensor(Epsilon)
Epsilon_prime <- torch_tensor(Epsilon_prime)
## Encoder for v_t
h <- w_1$mm(X_tensor)$add(b_1)$relu()
h_1 <- w_2$mm(h)$add(b_2)$relu()
sigma_sq_vec <- w_3$mm(h_1)$add(b_3)$exp()
mu <- w_4$mm(h_1)$add(b_4)$relu()
## Encoder for v_t_prime
h_prime <- w_1_prime$mm(X_tensor)$add(b_1_prime)$relu()
h_1_prime <- w_2_prime$mm(h_prime)$add(b_2_prime)$relu()
sigma_sq_vec_prime <- w_3_prime$mm(h_1_prime)$add(b_3_prime)$exp()
mu_prime <- w_4_prime$mm(h_1_prime)$add(b_4_prime)
## re-parameterization trick
v_t <- mu + sqrt(sigma_sq_vec)*Epsilon
v_t_prime <- mu_prime + sqrt(sigma_sq_vec_prime)*Epsilon_prime
# Decoder
l <- w_5$mm(v_t_prime)$add(b_5)$relu()
l_1 <- w_6$mm(l)$add(b_6)$relu()
theta_t <- w_7$mm(l_1)$add(b_7)$relu()
y_star <- W_tensor$pow(1/alpha)$mm(v_t)
# log_v <- torch_zeros(c(k,n.t))
# for (i in 1:n.t) {
#   for (j in 1:k) {
#     tmp = const1 * v_t[j,i]^{-const} * Zolo_vec * exp(-v_t[j,i]^(-const1) * Zolo_vec)
#     log_v[j,i] = -theta_t[j,i]^alpha+log(tmp$mean()) - theta_t[j,i]*v_t[j,i]
#   }
# }
V_t <- v_t$view(c(k*n.t,1))
Theta_t <- theta_t$view(c(k*n.t,1))
part_log_v1  <- V_t$pow(-const)$mm(Zolo_vec)
part_log_v2  <- (-V_t$pow(-const1)$mm(Zolo_vec))$exp()
part_log_v3 <- -Theta_t$pow(alpha)-Theta_t$mul(V_t)
part2 <- (part_log_v1$mul(part_log_v2)$mean(dim=2)$log()+part_log_v3$view(2500)$add(const3))$sum()
part1 = (X_tensor)$log()$sum()*const4 + y_star$log()$sum() - X_tensor$pow(const5)$mul(y_star)$sum()
# part2 = log_v$sum()                                                                        # p(v_t|theta_t), p(theta_t)
part4 = Epsilon$pow(2)$sum()/2 + Epsilon_prime$pow(2)$sum()/2 + sigma_sq_vec$log()$sum() + sigma_sq_vec_prime$log()$sum()
res <- part1 + part2 + part4
### -------- compute loss --------
loss <- (res/(n.s*n.t))
if(!is.finite(loss$item())) break
if (t %% 1 == 0)
cat("Epoch: ", t, "   ELBO: ", loss$item(), "\n") # we want to maximize
if (as.numeric(torch_isnan(loss))==1) break
### -------- Backpropagation --------
# compute gradient of loss w.r.t. all tensors with requires_grad = TRUE
loss$backward()
# w_1$grad$argmax(dim=2)
### -------- Update weights --------
# Wrap in with_no_grad() because this is a part we DON'T
# want to record for automatic gradient computation
with_no_grad({
# if(t>190 & learning_rate> -1e-7) {
#   learning_rate <- -1e-2
#   cat('Learing rate is now ', learning_rate, '\n')
# }
# if(t>2 & abs(loss$item()-old_loss)<0.002)  {
#   learning_rate <- learning_rate*alpha_v*0.1;
#   cat('Learing rate is now ', learning_rate, '\n')
# }
old_loss <- loss$item()
w_1_velocity <- alpha_v*w_1_velocity - learning_rate*w_1$grad
w_1$add_(w_1_velocity)
w_2_velocity <- alpha_v*w_2_velocity - learning_rate*w_2$grad
w_2$add_(w_2_velocity)
w_3_velocity <- alpha_v*w_3_velocity - learning_rate*w_3$grad
w_3$add_(w_3_velocity)
w_4_velocity <- alpha_v*w_4_velocity - learning_rate*w_4$grad
w_4$add_(w_4_velocity)
b_1_velocity <- alpha_v*b_1_velocity - learning_rate*b_1$grad
b_1$add_(b_1_velocity)
b_2_velocity <- alpha_v*b_2_velocity - learning_rate*b_2$grad
b_2$add_(b_2_velocity)
b_3_velocity <- alpha_v*b_3_velocity - learning_rate*b_3$grad
b_3$add_(b_3_velocity)
b_4_velocity <- alpha_v*b_4_velocity - learning_rate*b_4$grad
b_4$add_(b_4_velocity)
w_1_prime_velocity <- alpha_v*w_1_prime_velocity - learning_rate*w_1_prime$grad
w_1_prime$add_(w_1_prime_velocity)
w_2_prime_velocity <- alpha_v*w_2_prime_velocity - learning_rate*w_2_prime$grad
w_2_prime$add_(w_2_prime_velocity)
w_3_prime_velocity <- alpha_v*w_3_prime_velocity - learning_rate*w_3_prime$grad
w_3_prime$add_(w_3_prime_velocity)
w_4_prime_velocity <- alpha_v*w_4_prime_velocity - learning_rate*w_4_prime$grad
w_4_prime$add_(w_4_prime_velocity)
b_1_prime_velocity <- alpha_v*b_1_prime_velocity - learning_rate*b_1_prime$grad
b_1_prime$add_(b_1_prime_velocity)
b_2_prime_velocity <- alpha_v*b_2_prime_velocity - learning_rate*b_2_prime$grad
b_2_prime$add_(b_2_prime_velocity)
b_3_prime_velocity <- alpha_v*b_3_prime_velocity - learning_rate*b_3_prime$grad
b_3_prime$add_(b_3_prime_velocity)
b_4_prime_velocity <- alpha_v*b_4_prime_velocity - learning_rate*b_4_prime$grad
b_4_prime$add_(b_4_prime_velocity)
w_5_velocity <- alpha_v*w_5_velocity - learning_rate*w_5$grad
w_5$add_(w_5_velocity)
w_6_velocity <- alpha_v*w_6_velocity - learning_rate*w_6$grad
w_6$add_(w_6_velocity)
w_7_velocity <- alpha_v*w_7_velocity - learning_rate*w_7$grad
w_7$add_(w_7_velocity)
b_5_velocity <- alpha_v*b_5_velocity - learning_rate*b_5$grad
b_5$add_(b_5_velocity)
b_6_velocity <- alpha_v*b_6_velocity - learning_rate*b_6$grad
b_6$add_(b_6_velocity)
b_7_velocity <- alpha_v*b_7_velocity - learning_rate*b_7$grad
b_7$add_(b_7_velocity)
# Zero gradients after every pass, as they'd accumulate otherwise
w_1$grad$zero_()
w_2$grad$zero_()
w_3$grad$zero_()
w_4$grad$zero_()
b_1$grad$zero_()
b_2$grad$zero_()
b_3$grad$zero_()
b_4$grad$zero_()
w_1_prime$grad$zero_()
w_2_prime$grad$zero_()
w_3_prime$grad$zero_()
w_4_prime$grad$zero_()
b_1_prime$grad$zero_()
b_2_prime$grad$zero_()
b_3_prime$grad$zero_()
b_4_prime$grad$zero_()
w_5$grad$zero_()
w_6$grad$zero_()
w_7$grad$zero_()
b_5$grad$zero_()
b_6$grad$zero_()
b_7$grad$zero_()
})
}
learning_rate <- -5e-11
for (t in 1:niter) {
### -------- Forward pass --------
Epsilon <- t(0.1+abs(mvtnorm::rmvnorm(n.t, mean=rep(0, k), sigma = diag(rep(1, k)))))
Epsilon_prime <- t(mvtnorm::rmvnorm(n.t, mean=rep(0, k), sigma = diag(rep(1, k))))
Epsilon <- torch_tensor(Epsilon)
Epsilon_prime <- torch_tensor(Epsilon_prime)
## Encoder for v_t
h <- w_1$mm(X_tensor)$add(b_1)$relu()
h_1 <- w_2$mm(h)$add(b_2)$relu()
sigma_sq_vec <- w_3$mm(h_1)$add(b_3)$exp()
mu <- w_4$mm(h_1)$add(b_4)$relu()
## Encoder for v_t_prime
h_prime <- w_1_prime$mm(X_tensor)$add(b_1_prime)$relu()
h_1_prime <- w_2_prime$mm(h_prime)$add(b_2_prime)$relu()
sigma_sq_vec_prime <- w_3_prime$mm(h_1_prime)$add(b_3_prime)$exp()
mu_prime <- w_4_prime$mm(h_1_prime)$add(b_4_prime)
## re-parameterization trick
v_t <- mu + sqrt(sigma_sq_vec)*Epsilon
v_t_prime <- mu_prime + sqrt(sigma_sq_vec_prime)*Epsilon_prime
# Decoder
l <- w_5$mm(v_t_prime)$add(b_5)$relu()
l_1 <- w_6$mm(l)$add(b_6)$relu()
theta_t <- w_7$mm(l_1)$add(b_7)$relu()
y_star <- W_tensor$pow(1/alpha)$mm(v_t)
# log_v <- torch_zeros(c(k,n.t))
# for (i in 1:n.t) {
#   for (j in 1:k) {
#     tmp = const1 * v_t[j,i]^{-const} * Zolo_vec * exp(-v_t[j,i]^(-const1) * Zolo_vec)
#     log_v[j,i] = -theta_t[j,i]^alpha+log(tmp$mean()) - theta_t[j,i]*v_t[j,i]
#   }
# }
V_t <- v_t$view(c(k*n.t,1))
Theta_t <- theta_t$view(c(k*n.t,1))
part_log_v1  <- V_t$pow(-const)$mm(Zolo_vec)
part_log_v2  <- (-V_t$pow(-const1)$mm(Zolo_vec))$exp()
part_log_v3 <- -Theta_t$pow(alpha)-Theta_t$mul(V_t)
part2 <- (part_log_v1$mul(part_log_v2)$mean(dim=2)$log()+part_log_v3$view(2500)$add(const3))$sum()
part1 = (X_tensor)$log()$sum()*const4 + y_star$log()$sum() - X_tensor$pow(const5)$mul(y_star)$sum()
# part2 = log_v$sum()                                                                        # p(v_t|theta_t), p(theta_t)
part4 = Epsilon$pow(2)$sum()/2 + Epsilon_prime$pow(2)$sum()/2 + sigma_sq_vec$log()$sum() + sigma_sq_vec_prime$log()$sum()
res <- part1 + part2 + part4
### -------- compute loss --------
loss <- (res/(n.s*n.t))
if(!is.finite(loss$item())) break
if (t %% 1 == 0)
cat("Epoch: ", t, "   ELBO: ", loss$item(), "\n") # we want to maximize
if (as.numeric(torch_isnan(loss))==1) break
### -------- Backpropagation --------
# compute gradient of loss w.r.t. all tensors with requires_grad = TRUE
loss$backward()
# w_1$grad$argmax(dim=2)
### -------- Update weights --------
# Wrap in with_no_grad() because this is a part we DON'T
# want to record for automatic gradient computation
with_no_grad({
# if(t>190 & learning_rate> -1e-7) {
#   learning_rate <- -1e-2
#   cat('Learing rate is now ', learning_rate, '\n')
# }
# if(t>2 & abs(loss$item()-old_loss)<0.002)  {
#   learning_rate <- learning_rate*alpha_v*0.1;
#   cat('Learing rate is now ', learning_rate, '\n')
# }
old_loss <- loss$item()
w_1_velocity <- alpha_v*w_1_velocity - learning_rate*w_1$grad
w_1$add_(w_1_velocity)
w_2_velocity <- alpha_v*w_2_velocity - learning_rate*w_2$grad
w_2$add_(w_2_velocity)
w_3_velocity <- alpha_v*w_3_velocity - learning_rate*w_3$grad
w_3$add_(w_3_velocity)
w_4_velocity <- alpha_v*w_4_velocity - learning_rate*w_4$grad
w_4$add_(w_4_velocity)
b_1_velocity <- alpha_v*b_1_velocity - learning_rate*b_1$grad
b_1$add_(b_1_velocity)
b_2_velocity <- alpha_v*b_2_velocity - learning_rate*b_2$grad
b_2$add_(b_2_velocity)
b_3_velocity <- alpha_v*b_3_velocity - learning_rate*b_3$grad
b_3$add_(b_3_velocity)
b_4_velocity <- alpha_v*b_4_velocity - learning_rate*b_4$grad
b_4$add_(b_4_velocity)
w_1_prime_velocity <- alpha_v*w_1_prime_velocity - learning_rate*w_1_prime$grad
w_1_prime$add_(w_1_prime_velocity)
w_2_prime_velocity <- alpha_v*w_2_prime_velocity - learning_rate*w_2_prime$grad
w_2_prime$add_(w_2_prime_velocity)
w_3_prime_velocity <- alpha_v*w_3_prime_velocity - learning_rate*w_3_prime$grad
w_3_prime$add_(w_3_prime_velocity)
w_4_prime_velocity <- alpha_v*w_4_prime_velocity - learning_rate*w_4_prime$grad
w_4_prime$add_(w_4_prime_velocity)
b_1_prime_velocity <- alpha_v*b_1_prime_velocity - learning_rate*b_1_prime$grad
b_1_prime$add_(b_1_prime_velocity)
b_2_prime_velocity <- alpha_v*b_2_prime_velocity - learning_rate*b_2_prime$grad
b_2_prime$add_(b_2_prime_velocity)
b_3_prime_velocity <- alpha_v*b_3_prime_velocity - learning_rate*b_3_prime$grad
b_3_prime$add_(b_3_prime_velocity)
b_4_prime_velocity <- alpha_v*b_4_prime_velocity - learning_rate*b_4_prime$grad
b_4_prime$add_(b_4_prime_velocity)
w_5_velocity <- alpha_v*w_5_velocity - learning_rate*w_5$grad
w_5$add_(w_5_velocity)
w_6_velocity <- alpha_v*w_6_velocity - learning_rate*w_6$grad
w_6$add_(w_6_velocity)
w_7_velocity <- alpha_v*w_7_velocity - learning_rate*w_7$grad
w_7$add_(w_7_velocity)
b_5_velocity <- alpha_v*b_5_velocity - learning_rate*b_5$grad
b_5$add_(b_5_velocity)
b_6_velocity <- alpha_v*b_6_velocity - learning_rate*b_6$grad
b_6$add_(b_6_velocity)
b_7_velocity <- alpha_v*b_7_velocity - learning_rate*b_7$grad
b_7$add_(b_7_velocity)
# Zero gradients after every pass, as they'd accumulate otherwise
w_1$grad$zero_()
w_2$grad$zero_()
w_3$grad$zero_()
w_4$grad$zero_()
b_1$grad$zero_()
b_2$grad$zero_()
b_3$grad$zero_()
b_4$grad$zero_()
w_1_prime$grad$zero_()
w_2_prime$grad$zero_()
w_3_prime$grad$zero_()
w_4_prime$grad$zero_()
b_1_prime$grad$zero_()
b_2_prime$grad$zero_()
b_3_prime$grad$zero_()
b_4_prime$grad$zero_()
w_5$grad$zero_()
w_6$grad$zero_()
w_7$grad$zero_()
b_5$grad$zero_()
b_6$grad$zero_()
b_7$grad$zero_()
})
}
for (t in 1:niter) {
### -------- Forward pass --------
Epsilon <- t(0.1+abs(mvtnorm::rmvnorm(n.t, mean=rep(0, k), sigma = diag(rep(1, k)))))
Epsilon_prime <- t(mvtnorm::rmvnorm(n.t, mean=rep(0, k), sigma = diag(rep(1, k))))
Epsilon <- torch_tensor(Epsilon)
Epsilon_prime <- torch_tensor(Epsilon_prime)
## Encoder for v_t
h <- w_1$mm(X_tensor)$add(b_1)$relu()
h_1 <- w_2$mm(h)$add(b_2)$relu()
sigma_sq_vec <- w_3$mm(h_1)$add(b_3)$exp()
mu <- w_4$mm(h_1)$add(b_4)$relu()
## Encoder for v_t_prime
h_prime <- w_1_prime$mm(X_tensor)$add(b_1_prime)$relu()
h_1_prime <- w_2_prime$mm(h_prime)$add(b_2_prime)$relu()
sigma_sq_vec_prime <- w_3_prime$mm(h_1_prime)$add(b_3_prime)$exp()
mu_prime <- w_4_prime$mm(h_1_prime)$add(b_4_prime)
## re-parameterization trick
v_t <- mu + sqrt(sigma_sq_vec)*Epsilon
v_t_prime <- mu_prime + sqrt(sigma_sq_vec_prime)*Epsilon_prime
# Decoder
l <- w_5$mm(v_t_prime)$add(b_5)$relu()
l_1 <- w_6$mm(l)$add(b_6)$relu()
theta_t <- w_7$mm(l_1)$add(b_7)$relu()
y_star <- W_tensor$pow(1/alpha)$mm(v_t)
# log_v <- torch_zeros(c(k,n.t))
# for (i in 1:n.t) {
#   for (j in 1:k) {
#     tmp = const1 * v_t[j,i]^{-const} * Zolo_vec * exp(-v_t[j,i]^(-const1) * Zolo_vec)
#     log_v[j,i] = -theta_t[j,i]^alpha+log(tmp$mean()) - theta_t[j,i]*v_t[j,i]
#   }
# }
V_t <- v_t$view(c(k*n.t,1))
Theta_t <- theta_t$view(c(k*n.t,1))
part_log_v1  <- V_t$pow(-const)$mm(Zolo_vec)
part_log_v2  <- (-V_t$pow(-const1)$mm(Zolo_vec))$exp()
part_log_v3 <- -Theta_t$pow(alpha)-Theta_t$mul(V_t)
part2 <- (part_log_v1$mul(part_log_v2)$mean(dim=2)$log()+part_log_v3$view(2500)$add(const3))$sum()
part1 = (X_tensor)$log()$sum()*const4 + y_star$log()$sum() - X_tensor$pow(const5)$mul(y_star)$sum()
# part2 = log_v$sum()                                                                        # p(v_t|theta_t), p(theta_t)
part4 = Epsilon$pow(2)$sum()/2 + Epsilon_prime$pow(2)$sum()/2 + sigma_sq_vec$log()$sum() + sigma_sq_vec_prime$log()$sum()
res <- part1 + part2 + part4
### -------- compute loss --------
loss <- (res/(n.s*n.t))
if(!is.finite(loss$item())) break
if (t %% 1 == 0)
cat("Epoch: ", t, "   ELBO: ", loss$item(), "\n") # we want to maximize
if (as.numeric(torch_isnan(loss))==1) break
### -------- Backpropagation --------
# compute gradient of loss w.r.t. all tensors with requires_grad = TRUE
loss$backward()
# w_1$grad$argmax(dim=2)
### -------- Update weights --------
# Wrap in with_no_grad() because this is a part we DON'T
# want to record for automatic gradient computation
with_no_grad({
# if(t>190 & learning_rate> -1e-7) {
#   learning_rate <- -1e-2
#   cat('Learing rate is now ', learning_rate, '\n')
# }
# if(t>2 & abs(loss$item()-old_loss)<0.002)  {
#   learning_rate <- learning_rate*alpha_v*0.1;
#   cat('Learing rate is now ', learning_rate, '\n')
# }
old_loss <- loss$item()
w_1_velocity <- alpha_v*w_1_velocity - learning_rate*w_1$grad
w_1$add_(w_1_velocity)
w_2_velocity <- alpha_v*w_2_velocity - learning_rate*w_2$grad
w_2$add_(w_2_velocity)
w_3_velocity <- alpha_v*w_3_velocity - learning_rate*w_3$grad
w_3$add_(w_3_velocity)
w_4_velocity <- alpha_v*w_4_velocity - learning_rate*w_4$grad
w_4$add_(w_4_velocity)
b_1_velocity <- alpha_v*b_1_velocity - learning_rate*b_1$grad
b_1$add_(b_1_velocity)
b_2_velocity <- alpha_v*b_2_velocity - learning_rate*b_2$grad
b_2$add_(b_2_velocity)
b_3_velocity <- alpha_v*b_3_velocity - learning_rate*b_3$grad
b_3$add_(b_3_velocity)
b_4_velocity <- alpha_v*b_4_velocity - learning_rate*b_4$grad
b_4$add_(b_4_velocity)
w_1_prime_velocity <- alpha_v*w_1_prime_velocity - learning_rate*w_1_prime$grad
w_1_prime$add_(w_1_prime_velocity)
w_2_prime_velocity <- alpha_v*w_2_prime_velocity - learning_rate*w_2_prime$grad
w_2_prime$add_(w_2_prime_velocity)
w_3_prime_velocity <- alpha_v*w_3_prime_velocity - learning_rate*w_3_prime$grad
w_3_prime$add_(w_3_prime_velocity)
w_4_prime_velocity <- alpha_v*w_4_prime_velocity - learning_rate*w_4_prime$grad
w_4_prime$add_(w_4_prime_velocity)
b_1_prime_velocity <- alpha_v*b_1_prime_velocity - learning_rate*b_1_prime$grad
b_1_prime$add_(b_1_prime_velocity)
b_2_prime_velocity <- alpha_v*b_2_prime_velocity - learning_rate*b_2_prime$grad
b_2_prime$add_(b_2_prime_velocity)
b_3_prime_velocity <- alpha_v*b_3_prime_velocity - learning_rate*b_3_prime$grad
b_3_prime$add_(b_3_prime_velocity)
b_4_prime_velocity <- alpha_v*b_4_prime_velocity - learning_rate*b_4_prime$grad
b_4_prime$add_(b_4_prime_velocity)
w_5_velocity <- alpha_v*w_5_velocity - learning_rate*w_5$grad
w_5$add_(w_5_velocity)
w_6_velocity <- alpha_v*w_6_velocity - learning_rate*w_6$grad
w_6$add_(w_6_velocity)
w_7_velocity <- alpha_v*w_7_velocity - learning_rate*w_7$grad
w_7$add_(w_7_velocity)
b_5_velocity <- alpha_v*b_5_velocity - learning_rate*b_5$grad
b_5$add_(b_5_velocity)
b_6_velocity <- alpha_v*b_6_velocity - learning_rate*b_6$grad
b_6$add_(b_6_velocity)
b_7_velocity <- alpha_v*b_7_velocity - learning_rate*b_7$grad
b_7$add_(b_7_velocity)
# Zero gradients after every pass, as they'd accumulate otherwise
w_1$grad$zero_()
w_2$grad$zero_()
w_3$grad$zero_()
w_4$grad$zero_()
b_1$grad$zero_()
b_2$grad$zero_()
b_3$grad$zero_()
b_4$grad$zero_()
w_1_prime$grad$zero_()
w_2_prime$grad$zero_()
w_3_prime$grad$zero_()
w_4_prime$grad$zero_()
b_1_prime$grad$zero_()
b_2_prime$grad$zero_()
b_3_prime$grad$zero_()
b_4_prime$grad$zero_()
w_5$grad$zero_()
w_6$grad$zero_()
w_7$grad$zero_()
b_5$grad$zero_()
b_6$grad$zero_()
b_7$grad$zero_()
})
}
func1 <- function(x) {if(x<sqrt(3)) return(1-2*x/sqrt(12)) else return(0)}
func1 <- Vectorize(func1)
func2 <- function(x) {if(x<sqrt(1)) return(1+exp(-k-1)-exp(k-1)) else return(exp(-k-1))}
func2<- Vectorize(func2)
curve(func1, 0,5)
curve(func1, 0,5, col='red', lwd=2, ylab='Bounds', xlab='k')
curve(func2, 0,5, col='blue', lwd=2, add=TRUE)
func2 <- function(x) {if(x<=sqrt(1)) return(1+exp(-k-1)-exp(k-1)) else return(exp(-k-1))}
func2(0.5)
func2 <- function(x) {if(x<=sqrt(1)) return(1+exp(-x-1)-exp(x-1)) else return(exp(-x-1))}
func2<- Vectorize(func2)
curve(func1, 0,5, col='red', lwd=2, ylab='Bounds', xlab='k')
curve(func2, 0,5, col='blue', lwd=2, add=TRUE)
func2 <- function(x) {1/x^2}
curve(func2, 0,5, col='black', lwd=2,lty=2, add=TRUE)
legend('topright', lty=c(1,1,2), lwd=2, col=c('red','blue','black'), legend=c('Uniform', 'Exponential', 'Chebychev'))
